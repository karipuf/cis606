{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "#import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras import layers,optimizers\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "california=california_housing.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=california['data']\n",
    "y=california['target']\n",
    "\n",
    "x=(x-np.mean(x,axis=0))/np.std(x,axis=0)\n",
    "y=(y-np.mean(y))/np.std(y)\n",
    "\n",
    "#df=pd.DataFrame(x,columns=california['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest regressor\n",
    "\n",
    "* Benchmark system\n",
    "* RF generally gives great performance for problems like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18976576258571115"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE Error\n",
    "rf=RandomForestRegressor(n_estimators=100).fit(xtrain,ytrain)\n",
    "np.mean((rf.predict(xtest)-ytest)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Deep\" Neural Network\n",
    "\n",
    "* Two hidden layers\n",
    "* Investigate the effect of different params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18576 samples, validate on 2064 samples\n",
      "Epoch 1/100\n",
      "18576/18576 [==============================] - 1s - loss: 0.5740 - val_loss: 0.4081\n",
      "Epoch 2/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.4120 - val_loss: 0.3448\n",
      "Epoch 3/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3671 - val_loss: 0.3209\n",
      "Epoch 4/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3054\n",
      "Epoch 5/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3295 - val_loss: 0.2938\n",
      "Epoch 6/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3196 - val_loss: 0.2849\n",
      "Epoch 7/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3139 - val_loss: 0.2798\n",
      "Epoch 8/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3038 - val_loss: 0.2725\n",
      "Epoch 9/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3013 - val_loss: 0.2675\n",
      "Epoch 10/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2976 - val_loss: 0.2641\n",
      "Epoch 11/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2953 - val_loss: 0.2647\n",
      "Epoch 12/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2915 - val_loss: 0.2605\n",
      "Epoch 13/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2901 - val_loss: 0.2592\n",
      "Epoch 14/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2848 - val_loss: 0.2566\n",
      "Epoch 15/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2834 - val_loss: 0.2531\n",
      "Epoch 16/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2821 - val_loss: 0.2507\n",
      "Epoch 17/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2791 - val_loss: 0.2485\n",
      "Epoch 18/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2785 - val_loss: 0.2496\n",
      "Epoch 19/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2769 - val_loss: 0.2477\n",
      "Epoch 20/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2761 - val_loss: 0.2478\n",
      "Epoch 21/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2758 - val_loss: 0.2420\n",
      "Epoch 22/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2720 - val_loss: 0.2419\n",
      "Epoch 23/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2700 - val_loss: 0.2426\n",
      "Epoch 24/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2712 - val_loss: 0.2403\n",
      "Epoch 25/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2701 - val_loss: 0.2388\n",
      "Epoch 26/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2689 - val_loss: 0.2379\n",
      "Epoch 27/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2679 - val_loss: 0.2354\n",
      "Epoch 28/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2706 - val_loss: 0.2382\n",
      "Epoch 29/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2667 - val_loss: 0.2370\n",
      "Epoch 30/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2636 - val_loss: 0.2351\n",
      "Epoch 31/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2635 - val_loss: 0.2337\n",
      "Epoch 32/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2642 - val_loss: 0.2325\n",
      "Epoch 33/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2623 - val_loss: 0.2338\n",
      "Epoch 34/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2658 - val_loss: 0.2313\n",
      "Epoch 35/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2617 - val_loss: 0.2296\n",
      "Epoch 36/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2633 - val_loss: 0.2276\n",
      "Epoch 37/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2634 - val_loss: 0.2272\n",
      "Epoch 38/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2613 - val_loss: 0.2267\n",
      "Epoch 39/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2558 - val_loss: 0.2256\n",
      "Epoch 40/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2573 - val_loss: 0.2253\n",
      "Epoch 41/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2568 - val_loss: 0.2251\n",
      "Epoch 42/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2596 - val_loss: 0.2242\n",
      "Epoch 43/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2584 - val_loss: 0.2241\n",
      "Epoch 44/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2580 - val_loss: 0.2229\n",
      "Epoch 45/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2554 - val_loss: 0.2226\n",
      "Epoch 46/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2605 - val_loss: 0.2233\n",
      "Epoch 47/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2561 - val_loss: 0.2218\n",
      "Epoch 48/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2567 - val_loss: 0.2213\n",
      "Epoch 49/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2540 - val_loss: 0.2207\n",
      "Epoch 50/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2552 - val_loss: 0.2198\n",
      "Epoch 51/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2536 - val_loss: 0.2196\n",
      "Epoch 52/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2550 - val_loss: 0.2192\n",
      "Epoch 53/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2536 - val_loss: 0.2196\n",
      "Epoch 54/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2537 - val_loss: 0.2193\n",
      "Epoch 55/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2531 - val_loss: 0.2193\n",
      "Epoch 56/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2503 - val_loss: 0.2192\n",
      "Epoch 57/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2516 - val_loss: 0.2187\n",
      "Epoch 58/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2546 - val_loss: 0.2181\n",
      "Epoch 59/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2496 - val_loss: 0.2172\n",
      "Epoch 60/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2530 - val_loss: 0.2171\n",
      "Epoch 61/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2511 - val_loss: 0.2163\n",
      "Epoch 62/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2503 - val_loss: 0.2163\n",
      "Epoch 63/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2517 - val_loss: 0.2170\n",
      "Epoch 64/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2497 - val_loss: 0.2163\n",
      "Epoch 65/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2485 - val_loss: 0.2159\n",
      "Epoch 66/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2488 - val_loss: 0.2154\n",
      "Epoch 67/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2495 - val_loss: 0.2153\n",
      "Epoch 68/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2492 - val_loss: 0.2153\n",
      "Epoch 69/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2505 - val_loss: 0.2153\n",
      "Epoch 70/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2503 - val_loss: 0.2151\n",
      "Epoch 71/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2492 - val_loss: 0.2147\n",
      "Epoch 72/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2491 - val_loss: 0.2144\n",
      "Epoch 73/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2493 - val_loss: 0.2154\n",
      "Epoch 74/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2492 - val_loss: 0.2145\n",
      "Epoch 75/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2497 - val_loss: 0.2144\n",
      "Epoch 76/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2505 - val_loss: 0.2142\n",
      "Epoch 77/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2486 - val_loss: 0.2138\n",
      "Epoch 78/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2477 - val_loss: 0.2142\n",
      "Epoch 79/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2463 - val_loss: 0.2137\n",
      "Epoch 80/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2470 - val_loss: 0.2138\n",
      "Epoch 81/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2452 - val_loss: 0.2128\n",
      "Epoch 82/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2447 - val_loss: 0.2126\n",
      "Epoch 83/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2437 - val_loss: 0.2116\n",
      "Epoch 84/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2480 - val_loss: 0.2112\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18576/18576 [==============================] - 0s - loss: 0.2459 - val_loss: 0.2113\n",
      "Epoch 86/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2466 - val_loss: 0.2113\n",
      "Epoch 87/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2487 - val_loss: 0.2111\n",
      "Epoch 88/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2448 - val_loss: 0.2108\n",
      "Epoch 89/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2442 - val_loss: 0.2109\n",
      "Epoch 90/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2461 - val_loss: 0.2110\n",
      "Epoch 91/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2443 - val_loss: 0.2104\n",
      "Epoch 92/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2460 - val_loss: 0.2104\n",
      "Epoch 93/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2450 - val_loss: 0.2100\n",
      "Epoch 94/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2471 - val_loss: 0.2097\n",
      "Epoch 95/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2457 - val_loss: 0.2094\n",
      "Epoch 96/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2457 - val_loss: 0.2097\n",
      "Epoch 97/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2431 - val_loss: 0.2098\n",
      "Epoch 98/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2460 - val_loss: 0.2095\n",
      "Epoch 99/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2435 - val_loss: 0.2094\n",
      "Epoch 100/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2448 - val_loss: 0.2095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b02dce978>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential API\n",
    "\n",
    "net=Sequential()\n",
    "\n",
    "net.add(layers.Dense(256,input_shape=(8,)))\n",
    "net.add(layers.Activation('relu'))\n",
    "net.add(layers.Dropout(rate=.2))\n",
    "for nUnits in [160,64]:\n",
    "    net.add(layers.Dense(nUnits,input_shape=(8,)))\n",
    "    net.add(layers.Activation('relu'))\n",
    "    #net.add(layers.BatchNormalization())\n",
    "    net.add(layers.Dropout(rate=.2))\n",
    "net.add(layers.Dense(1))\n",
    "\n",
    "opt=optimizers.Adam(lr=.01,decay=.1)\n",
    "net.compile(opt,loss='mse')\n",
    "\n",
    "net.fit(xtrain,ytrain,batch_size=1000,validation_data=(xtest,ytest),epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18576 samples, validate on 2064 samples\n",
      "Epoch 1/100\n",
      "18576/18576 [==============================] - 1s - loss: 0.5186 - val_loss: 0.4192\n",
      "Epoch 2/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3720 - val_loss: 0.3271\n",
      "Epoch 3/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3428 - val_loss: 0.3016\n",
      "Epoch 4/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3233 - val_loss: 0.2819\n",
      "Epoch 5/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3101 - val_loss: 0.2760\n",
      "Epoch 6/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.3010 - val_loss: 0.2668\n",
      "Epoch 7/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2990 - val_loss: 0.2664\n",
      "Epoch 8/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2924 - val_loss: 0.2644\n",
      "Epoch 9/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2873 - val_loss: 0.2554\n",
      "Epoch 10/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2831 - val_loss: 0.2550\n",
      "Epoch 11/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2847 - val_loss: 0.2536\n",
      "Epoch 12/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2772 - val_loss: 0.2482\n",
      "Epoch 13/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2760 - val_loss: 0.2449\n",
      "Epoch 14/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2765 - val_loss: 0.2431\n",
      "Epoch 15/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2726 - val_loss: 0.2405\n",
      "Epoch 16/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2716 - val_loss: 0.2395\n",
      "Epoch 17/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2697 - val_loss: 0.2392\n",
      "Epoch 18/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2675 - val_loss: 0.2374\n",
      "Epoch 19/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2630 - val_loss: 0.2358\n",
      "Epoch 20/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2652 - val_loss: 0.2371\n",
      "Epoch 21/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2631 - val_loss: 0.2353\n",
      "Epoch 22/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2627 - val_loss: 0.2389\n",
      "Epoch 23/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2628 - val_loss: 0.2381\n",
      "Epoch 24/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2580 - val_loss: 0.2405\n",
      "Epoch 25/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2595 - val_loss: 0.2416\n",
      "Epoch 26/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2592 - val_loss: 0.2369\n",
      "Epoch 27/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2595 - val_loss: 0.2349\n",
      "Epoch 28/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2592 - val_loss: 0.2300\n",
      "Epoch 29/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2588 - val_loss: 0.2275\n",
      "Epoch 30/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2546 - val_loss: 0.2262\n",
      "Epoch 31/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2559 - val_loss: 0.2254\n",
      "Epoch 32/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2553 - val_loss: 0.2254\n",
      "Epoch 33/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2523 - val_loss: 0.2239\n",
      "Epoch 34/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2526 - val_loss: 0.2236\n",
      "Epoch 35/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2526 - val_loss: 0.2235\n",
      "Epoch 36/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2534 - val_loss: 0.2241\n",
      "Epoch 37/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2510 - val_loss: 0.2253\n",
      "Epoch 38/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2526 - val_loss: 0.2234\n",
      "Epoch 39/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2505 - val_loss: 0.2220\n",
      "Epoch 40/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2495 - val_loss: 0.2207\n",
      "Epoch 41/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2491 - val_loss: 0.2199\n",
      "Epoch 42/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2485 - val_loss: 0.2192\n",
      "Epoch 43/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2510 - val_loss: 0.2193\n",
      "Epoch 44/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2478 - val_loss: 0.2187\n",
      "Epoch 45/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2481 - val_loss: 0.2189\n",
      "Epoch 46/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2495 - val_loss: 0.2180\n",
      "Epoch 47/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2482 - val_loss: 0.2188\n",
      "Epoch 48/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2469 - val_loss: 0.2181\n",
      "Epoch 49/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2468 - val_loss: 0.2167\n",
      "Epoch 50/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2473 - val_loss: 0.2164\n",
      "Epoch 51/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2447 - val_loss: 0.2172\n",
      "Epoch 52/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2463 - val_loss: 0.2169\n",
      "Epoch 53/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2439 - val_loss: 0.2167\n",
      "Epoch 54/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2445 - val_loss: 0.2153\n",
      "Epoch 55/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2456 - val_loss: 0.2153\n",
      "Epoch 56/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2443 - val_loss: 0.2151\n",
      "Epoch 57/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2463 - val_loss: 0.2153\n",
      "Epoch 58/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2458 - val_loss: 0.2157\n",
      "Epoch 59/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2425 - val_loss: 0.2149\n",
      "Epoch 60/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2441 - val_loss: 0.2138\n",
      "Epoch 61/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2436 - val_loss: 0.2135\n",
      "Epoch 62/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2446 - val_loss: 0.2133\n",
      "Epoch 63/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2432 - val_loss: 0.2120\n",
      "Epoch 64/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2443 - val_loss: 0.2127\n",
      "Epoch 65/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2439 - val_loss: 0.2128\n",
      "Epoch 66/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2431 - val_loss: 0.2126\n",
      "Epoch 67/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2412 - val_loss: 0.2126\n",
      "Epoch 68/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2432 - val_loss: 0.2129\n",
      "Epoch 69/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2431 - val_loss: 0.2133\n",
      "Epoch 70/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2407 - val_loss: 0.2123\n",
      "Epoch 71/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2409 - val_loss: 0.2108\n",
      "Epoch 72/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2417 - val_loss: 0.2112\n",
      "Epoch 73/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2413 - val_loss: 0.2109\n",
      "Epoch 74/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2418 - val_loss: 0.2107\n",
      "Epoch 75/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2416 - val_loss: 0.2101\n",
      "Epoch 76/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2423 - val_loss: 0.2094\n",
      "Epoch 77/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2392 - val_loss: 0.2098\n",
      "Epoch 78/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2396 - val_loss: 0.2102\n",
      "Epoch 79/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2402 - val_loss: 0.2097\n",
      "Epoch 80/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2398 - val_loss: 0.2094\n",
      "Epoch 81/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2391 - val_loss: 0.2100\n",
      "Epoch 82/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2398 - val_loss: 0.2088\n",
      "Epoch 83/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2383 - val_loss: 0.2089\n",
      "Epoch 84/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2405 - val_loss: 0.2089\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18576/18576 [==============================] - 0s - loss: 0.2408 - val_loss: 0.2089\n",
      "Epoch 86/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2374 - val_loss: 0.2095\n",
      "Epoch 87/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2376 - val_loss: 0.2094\n",
      "Epoch 88/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2371 - val_loss: 0.2086\n",
      "Epoch 89/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2371 - val_loss: 0.2083\n",
      "Epoch 90/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2366 - val_loss: 0.2086\n",
      "Epoch 91/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2361 - val_loss: 0.2083\n",
      "Epoch 92/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2369 - val_loss: 0.2080\n",
      "Epoch 93/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2397 - val_loss: 0.2074\n",
      "Epoch 94/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2372 - val_loss: 0.2070\n",
      "Epoch 95/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2359 - val_loss: 0.2069\n",
      "Epoch 96/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2372 - val_loss: 0.2071\n",
      "Epoch 97/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2363 - val_loss: 0.2073\n",
      "Epoch 98/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2359 - val_loss: 0.2067\n",
      "Epoch 99/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2360 - val_loss: 0.2067\n",
      "Epoch 100/100\n",
      "18576/18576 [==============================] - 0s - loss: 0.2373 - val_loss: 0.2065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b003203c8>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional API\n",
    "\n",
    "inp=layers.Input(shape=(8,))\n",
    "l1=layers.Dense(256)\n",
    "d1=l1(inp)\n",
    "d1r=layers.Activation('relu')(d1)\n",
    "d1r=layers.Dropout(rate=.2)(d1r)\n",
    "d2=layers.Dense(160)(d1r)\n",
    "d2r=layers.Activation('relu')(d2)\n",
    "d2r=layers.Dropout(rate=.2)(d2r)\n",
    "d3=layers.Dense(64)(d2r)\n",
    "d3r=layers.Activation('relu')(d3)\n",
    "d3r=layers.Dropout(rate=.2)(d3r)\n",
    "predictions=layers.Dense(1)(d3r)\n",
    "\n",
    "mod=Model(inputs=inp,outputs=predictions)\n",
    "mod.compile(optimizers.Adam(lr=.01,decay=.1),loss='mse')\n",
    "mod.fit(xtrain,ytrain,batch_size=1000,validation_data=(xtest,ytest),epochs=100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
